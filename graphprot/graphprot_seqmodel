#!/usr/bin/env python

import argparse
import logging
import os
import sys
from collections import deque
from eden.model_base import ModelInitializerBase, main_script
from eden.sequence import Vectorizer
from eden.util import configure_logging
from eden.util import save_output
from eden.util import serialize_dict
from GArDen.convert.sequence import FastaToSeq
from numpy.random import randint, uniform
from sklearn.linear_model import SGDClassifier
from time import time

logger = logging.getLogger(__name__)

DESCRIPTION = """
GraphProt:

Explicit Decomposition with Neighborhood (EDeN) utility program for
sequence-based modeling of binding preferences of RNA-binding proteins.
"""

EPILOG = """
Author: Daniel Maticzka
Copyright: 2015
License: GPL
Maintainer: Daniel Maticzka
Email: maticzkd@informatik.uni-freiburg.de
Status: Development

Please cite:

    Daniel Maticzka, Sita J. Lange, Fabrizio Costa, Rolf Backofen
    GraphProt: modeling binding preferences of RNA-binding proteins
    Genome Biology, 2014, 15(1), R17
"""


class ModelInitializer(ModelInitializerBase):

    def _load_data(self, input_file):
        """Load fasta and transform to seq."""
        seqs = FastaToSeq(normalize=False).transform(input_file)
        return seqs

    def load_data(self, args):
        """Load and convert positive class data."""
        return self._load_data(args.input_file)

    def load_positive_data(self, args):
        """Load and convert positive class data."""
        return self._load_data(args.positive_input_file)

    def load_negative_data(self, args):
        """Load and convert negative class data."""
        return self._load_data(args.negative_input_file)

    def pre_processor_init(self, args):
        """Setup preprocessing of data prepared by load_data functions.

        Returns the function used to process the data prepared by the load_data functions and
        a set of matching parameter choices.
        """
        def pre_processor(seqs, **args):
            vp_weight = args["vp_weight"]
            context_weight = args["context_weight"]
            if "priors" in args:
                priorstable = args["priors"]
                priors_weight = args["priors_weight"]
            for id, seq in seqs:
                # calculate list of viewpoint weights
                weights = map(lambda x: vp_weight if x.isupper() else context_weight, seq)
                # normalize RNA sequence
                seq = seq.upper().replace('T', 'U')
                # calculate list of RNase T1 cleavage probabilities
                if "priors" in args:
                    priors = deque()
                    # sniff kmer length from first dictionary key
                    kmer_len = len(priorstable.keys()[0])
                    # look up probabilities
                    for i in range(len(seq)):
                        try:
                            prob = priorstable[seq[i:(i + kmer_len)]]
                        except KeyError:
                            prob = None
                        priors.append(prob)
                    # compensate for the fact that the probabilities relate
                    # to the center positions of the kmers
                    priors.rotate(int(kmer_len / 2))
                    priors = list(priors)
                    # combine weights and priors
                    comb = []
                    for w, p in zip(weights, priors):
                        if p is not None:
                            comb.append(max(0.0000000001, w - (priors_weight * p)))
                        else:
                            comb.append(0.0000000001)
                    weights = comb
                yield id, seq, weights

        pre_processor_parameters = {"vp_weight": [1.0],
                                    "context_weight": [0.0000000001]}

        if (args.priors_file is not None):
            # add k-mer probabilities to parameters
            csv = open(args.priors_file, "r")
            priors = dict(map(lambda s: [s.split()[0], float(s.split()[1])], csv))
            pre_processor_parameters["priors"] = [priors]
            pre_processor_parameters["priors_weight"] = [args.priors_weight]

        return pre_processor, pre_processor_parameters

    def vectorizer_init(self, args):
        """Setup conversion of graphs to features."""
        vectorizer = Vectorizer()
        vectorizer_parameters = {
            'r': [1, 0, 2, 3, 4],
            'd': [4, 0, 1, 2, 3, 4, 5, 6],
            'nbits': [14, 16, 18]}
        return vectorizer, vectorizer_parameters

    def estimator_init(self, args):
        """Setup the estimator and set of matching parameter choices."""
        # defaults to first entry of each list if no parameter optimization is done
        # set to defaults of SGDClassifier here
        estimator = SGDClassifier(average=True, class_weight='balanced', shuffle=True)
        estimator_parameters = {'n_iter': [5] + list(randint(5, 200, size=args.n_iter)),
                                'penalty': ['l2', 'l1', 'elasticnet'],
                                'l1_ratio': [0.15] + list(uniform(0.1, 0.9, size=args.n_iter)),
                                'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
                                'power_t': [0.5] + list(uniform(0.1, size=args.n_iter)),
                                'alpha': [0.0001] + [10 ** x for x in range(-8, 0)],
                                'eta0': [10 ** x for x in range(-4, -1)],
                                'learning_rate': ["optimal", "invscaling", "constant"],
                                'n_jobs': [-1]}
        return estimator, estimator_parameters

    def add_arguments(self, parser):
        """Add arguments for the main call."""
        parser.add_argument('--version', action='version', version='0.1')
        return parser

    def add_arguments_fit(self, parser):
        """Add arguments for the fit command."""
        parser.add_argument("-p", "--bound-file",
                            dest="positive_input_file",
                            help="Path to FASTA file containing sequences of bound sites.",
                            required=True)
        parser.add_argument("-n", "--unbound-file",
                            dest="negative_input_file",
                            help="Path to FASTA file containing sequences on unbound sites.",
                            required=True)
        parser.add_argument("--kmer-probs",
                            dest="priors_file",
                            default=None,
                            help="Path to tab separated file containing two columns of k-mers and associated probabilities.")
        parser.add_argument("--kmer-weight",
                            dest="priors_weight",
                            type=float,
                            default=1.0,
                            help="Weight given to the k-mer probabilities.")
        return parser

    def add_arguments_estimate(self, parser):
        """Add arguments for the estimate command."""
        return self.add_arguments_fit(parser)

    def add_arguments_base(self, parser):
        parser.add_argument("-i", "--input-file",
                            dest="input_file",
                            help="Path to FASTA file containing input sequences.",
                            required=True)
        return parser

    def add_arguments_matrix(self, parser):
        """Add arguments used by the matrix command."""
        return parser

    def add_arguments_predict(self, parser):
        """Add arguments used by the predict command."""
        return parser

    def add_arguments_feature(self, parser):
        """Add arguments used by the feature command."""
        return parser


def main_predict_profile(model_initializer, args):
    iterator = model_initializer.load_data(args)
    from itertools import tee
    iterator, iterator_ = tee(iterator)

    from eden.model import ActiveLearningBinaryClassificationModel
    model = ActiveLearningBinaryClassificationModel()
    model.load(args.model_file)
    logger.info(model.get_parameters())

    output = []
    g = model.annotate(iterator)
    from itertools import izip
    for (header, seq), (vertices, scores) in izip(iterator_, g):
        assert "".join(vertices) == seq.upper().replace('T', 'U'), "error: pre_processor was not applied. got raw sequence '{}'".format(seq)
        for pos, score in enumerate(scores):
            output.append("{}\t{}\t{}\n".format(header, pos, score))
    save_output(text=output, output_dir_path=args.output_dir_path, out_file_name='profile.txt')


if __name__ == "__main__":
    if "predict_profile" in sys.argv:
        parser = argparse.ArgumentParser()
        parser.add_argument("-v", "--verbosity",
                            action="count",
                            help="Increase output verbosity")
        parser.add_argument("-x", "--no-logging",
                            dest="no_logging",
                            help="If set, do not log on file.",
                            action="store_true")
        parser.add_argument(
            "action",
            help="What to do.")
        parser.add_argument("-i", "--input-file",
                            dest="input_file",
                            help="Path to file containing input.",
                            required=True)
        parser.add_argument("-m", "--model-file",
                            dest="model_file",
                            help="Model file name. Note: it will be located in the output directory.",
                            default="model")
        parser.add_argument("-o", "--output-dir",
                            dest="output_dir_path",
                            help="Path to output directory.",
                            default="out")
        args = parser.parse_args()
        assert args.action == "predict_profile"
        model_initializer = ModelInitializer()

        prog_name = "graphprot_seqmodel"
        if args.no_logging:
            configure_logging(logger, verbosity=args.verbosity)
        else:
            configure_logging(logger, verbosity=args.verbosity, filename=prog_name + '.log')

        logger.debug('-' * 80)
        logger.debug('Program: %s' % prog_name)
        logger.debug('Called with parameters:\n %s' % serialize_dict(args.__dict__))

        start_time = time()
        try:
            main_predict_profile(model_initializer, args)
        except Exception:
            import datetime
            curr_time = datetime.datetime.now().strftime("%A, %d. %B %Y %I:%M%p")
            logger.exception("Program run failed on %s" % curr_time)
            exit(1)
        finally:
            end_time = time()
            logger.info('Elapsed time: %.1f sec', end_time - start_time)
        exit(0)
    else:
        model_initializer = ModelInitializer()
        main_script(model_initializer=model_initializer,
                    description=DESCRIPTION,
                    epilog=EPILOG,
                    prog_name=os.path.basename(__file__),
                    logger=logging.getLogger())
