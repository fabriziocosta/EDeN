{
 "metadata": {
  "name": "",
  "signature": "sha256:63561073f8c4f6c3ffc0ab11aee4ef90ffeeee223153f476ad1af05fa2d65f31"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sequence Modeling with EDeN\n",
      "##The case for real valued vector labels\n",
      "\n",
      "**Aim:** Suppose you are given two sets of sequences. Each sequence is composed of characters in a finite alphabet. However there are similarity relationships between the characters. We want to build a predictive model that can discriminate between the two sets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Artificial Dataset\n",
      "\n",
      "Lets build an artificial case. We construct two classes in the following way: for each class we start from a specific but random seed sequence, and the full set is then generated every time by permuting the position of k pairs of characters chosen at random in the seed sequence.\n",
      "\n",
      "To simulate the relationship between characters we do as follows: we select at random some charaters and we capitalize them. For the machine, a capitalized character is completely different from its lowercase counterpart, but it is easier for humans to see them. \n",
      "\n",
      "Assume the similarity between chars is given as a symmetric matrix. We can then perform a low dimensionality embedding of the similarity matrix (e.g. MDS in $\\mathbb{R}^4$) and obtain some vector representation for each char such that their euclidean distance is proportional to their dissimilarity. Lets assume we are already given the vector representation. In our case we just take some random vectors as they will be roughly equally distant from each other. In order to simulate that the capitalized version of a cahr should be similar to its lowercase counterpart, we just add a small amount of noise to the vector representation of one of the two.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Auxiliary Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code for making artificial dataset\n",
      "import random\n",
      "\n",
      "def swap_two_characters(seq):\n",
      "    '''define a function that swaps two characters at random positions in a string '''\n",
      "    line = list(seq)\n",
      "    id_i = random.randint(0,len(line)-1)\n",
      "    id_j = random.randint(0,len(line)-1)\n",
      "    line[id_i], line[id_j] = line[id_j], line[id_i]\n",
      "    return ''.join(line)\n",
      "\n",
      "def swap_characters(seed, n):\n",
      "    seq=seed\n",
      "    for i in range(n):\n",
      "        seq = swap_two_characters(seq)\n",
      "    return seq\n",
      "    \n",
      "def make_seed(start=0, end=26):\n",
      "    seq = ''.join([str(unichr(97+i)) for i in range(start,end)])\n",
      "    return swap_characters(seq, end-start)\n",
      "    \n",
      "def make_dataset(n_sequences=None, seed=None, n_swaps=None):\n",
      "    seqs = []\n",
      "    seqs.append( seed )\n",
      "    for i in range(n_sequences):\n",
      "        seq = swap_characters( seed, n_swaps )\n",
      "        seqs.append( seq )        \n",
      "    return seqs\n",
      "\n",
      "def random_capitalize(seqs, p=0.5):\n",
      "    new_seqs=[]\n",
      "    for seq in seqs:\n",
      "        new_seq = [c.upper() if random.random() < p else c for c in seq ]\n",
      "        new_seqs.append(''.join(new_seq))\n",
      "    return new_seqs\n",
      "\n",
      "def make_artificial_dataset(sequence_length=None, n_sequences=None, n_swaps=None):\n",
      "    seed = make_seed(start=0, end=sequence_length)\n",
      "    print 'Seed: ',seed\n",
      "    seqs = make_dataset(n_sequences=n_sequences, seed=seed, n_swaps=n_swaps)\n",
      "    train_seqs_orig=seqs[:len(seqs)/2]\n",
      "    test_seqs_orig=seqs[len(seqs)/2:]\n",
      "    seqs = random_capitalize(seqs, p=0.5)\n",
      "    print 'Sample with random capitalization:',seqs[:7]\n",
      "    train_seqs=seqs[:len(seqs)/2]\n",
      "    test_seqs=seqs[len(seqs)/2:]\n",
      "    return train_seqs_orig, test_seqs_orig, train_seqs, test_seqs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code to estimate predictive performance on categorical labeled sequences\n",
      "\n",
      "def discriminative_estimate(train_pos_seqs, train_neg_seqs, test_pos_seqs, test_neg_seqs):\n",
      "    from eden.graph import Vectorizer\n",
      "    vectorizer = Vectorizer(complexity=complexity)\n",
      "\n",
      "    from eden.converter.graph.sequence import sequence_to_eden\n",
      "    iterable_pos = sequence_to_eden(train_pos_seqs)\n",
      "    iterable_neg = sequence_to_eden(train_neg_seqs)\n",
      "\n",
      "    from eden.util import  fit, estimate\n",
      "    estimator = fit(iterable_pos,iterable_neg, vectorizer, n_iter_search=n_iter_search)\n",
      "\n",
      "    from eden.converter.graph.sequence import sequence_to_eden\n",
      "    iterable_pos = sequence_to_eden(test_pos_seqs)\n",
      "    iterable_neg = sequence_to_eden(test_neg_seqs)\n",
      "    estimate(iterable_pos, iterable_neg, estimator, vectorizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code to create real vector labels\n",
      "def make_encoding(encoding_vector_dimension=3, sequence_length=None, noise_size=0.01):\n",
      "    #vector encoding for chars\n",
      "    default_encoding = [0]*encoding_vector_dimension\n",
      "    start=0\n",
      "    end=sequence_length\n",
      "    #take a list of all chars up to 'length' \n",
      "    char_list = [str(unichr(97+i)) for i in range(start,end)]\n",
      "\n",
      "    encodings={}\n",
      "    import numpy as np\n",
      "    codes = np.random.rand(len(char_list),encoding_vector_dimension)\n",
      "    for i, code in enumerate(codes):\n",
      "        c = str(unichr(97+i))\n",
      "        cc = c.upper()\n",
      "        encoding = list(code)\n",
      "        encodings[c] = encoding\n",
      "        #add noise for the encoding of capitalized chars\n",
      "        noise = np.random.rand(encoding_vector_dimension)*noise_size\n",
      "        encodings[cc] = list(code + noise)\n",
      "    return encodings, default_encoding\n",
      "\n",
      "def make_encodings(n_encodings=3, encoding_vector_dimension=3, sequence_length=None, noise_size=0.01):\n",
      "    encodings=[]\n",
      "    for i in range(1,n_encodings+1):\n",
      "        encoding, default_encoding = make_encoding(encoding_vector_dimension, sequence_length, noise_size=noise_size)\n",
      "        encodings.append(encoding)\n",
      "    return encodings, default_encoding"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Artificial data generation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#problem parameters\n",
      "random.seed(1)\n",
      "sequence_length = 8 #sequences length\n",
      "n_sequences = 50    #num sequences in positive and negative set\n",
      "n_swaps = 2         #num pairs of chars that are swapped at random\n",
      "n_iter_search = 30  #num paramter configurations that are evaluated in hyperparameter optimization\n",
      "complexity = 2      #feature complexity for the vectorizer \n",
      "n_encodings = 5     #num vector encoding schemes for chars\n",
      "encoding_vector_dimension = 9 #vector dimension for char encoding\n",
      "noise_size = 0.05   #amount of random noise "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Positive examples:'\n",
      "train_pos_seqs_orig, test_pos_seqs_orig, train_pos_seqs, test_pos_seqs = make_artificial_dataset(sequence_length,n_sequences,n_swaps)\n",
      "print 'Negative examples:'\n",
      "train_neg_seqs_orig, test_neg_seqs_orig, train_neg_seqs, test_neg_seqs = make_artificial_dataset(sequence_length,n_sequences,n_swaps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Positive examples:\n",
        "Seed:  dgbcefah\n",
        "Sample with random capitalization: ['DgBcEFaH', 'ghbCEFad', 'egBhDFAc', 'cdBgeFaH', 'DGbceFAh', 'bCdGEfAH', 'dFBCAgEh']\n",
        "Negative examples:\n",
        "Seed:  afbdhgce\n",
        "Sample with random capitalization: ['AfbdhGce', 'afcdeGBh', 'dfBGhacE', 'aFBDhGCE', 'ahbcfgde', 'afGdEBch', 'efBdhgcA']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Discriminative model on categorical labels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#lets estimate the predictive performance of a classifier over the original sequences\n",
      "print 'Predictive performance on original sequences'\n",
      "discriminative_estimate(train_pos_seqs_orig, train_neg_seqs_orig, test_pos_seqs_orig, test_neg_seqs_orig)\n",
      "print '\\n\\n'\n",
      "#lets estimate the predictive performance of a classifier over the capitalized sequences\n",
      "print 'Predictive performance on sequences with random capitalization'\n",
      "discriminative_estimate(train_pos_seqs, train_neg_seqs, test_pos_seqs, test_neg_seqs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predictive performance on original sequences\n",
        "Classifier:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SGDClassifier(alpha=0.000801360522606, class_weight='auto', epsilon=0.1,\n",
        "       eta0=0.264967379258, fit_intercept=True, l1_ratio=0.15,\n",
        "       learning_rate='constant', loss='hinge', n_iter=55, n_jobs=1,\n",
        "       penalty='l1', power_t=0.566290835485, random_state=None,\n",
        "       shuffle=True, verbose=0, warm_start=False)\n",
        "--------------------------------------------------------------------------------\n",
        "Predictive performance:\n",
        "            accuracy: 0.917 +- 0.154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "           precision: 0.875 +- 0.301"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              recall: 0.867 +- 0.306"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n",
        "/Library/Python/2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                  f1: 0.866 +- 0.297"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   average_precision: 1.000 +- 0.000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             roc_auc: 1.000 +- 0.000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------------------------------------------------\n",
        "Test set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Instances: 52 ; Features: 1048577 with an avg of 63 features per instance\n",
        "--------------------------------------------------------------------------------\n",
        "Test Estimate\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "         -1       0.85      0.85      0.85        26\n",
        "          1       0.85      0.85      0.85        26\n",
        "\n",
        "avg / total       0.85      0.85      0.85        52\n",
        "\n",
        "APR: 0.930\n",
        "ROC: 0.932\n",
        "\n",
        "\n",
        "\n",
        "Predictive performance on sequences with random capitalization\n",
        "Classifier:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SGDClassifier(alpha=0.000922378874419, class_weight='auto', epsilon=0.1,\n",
        "       eta0=0.344370675484, fit_intercept=True, l1_ratio=0.15,\n",
        "       learning_rate='optimal', loss='hinge', n_iter=33, n_jobs=1,\n",
        "       penalty='l1', power_t=1.01457362778, random_state=None,\n",
        "       shuffle=True, verbose=0, warm_start=False)\n",
        "--------------------------------------------------------------------------------\n",
        "Predictive performance:\n",
        "            accuracy: 0.792 +- 0.159"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "           precision: 0.837 +- 0.204"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              recall: 0.817 +- 0.229"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                  f1: 0.793 +- 0.158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   average_precision: 0.906 +- 0.121"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             roc_auc: 0.856 +- 0.170"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------------------------------------------------\n",
        "Test set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Instances: 52 ; Features: 1048577 with an avg of 63 features per instance\n",
        "--------------------------------------------------------------------------------\n",
        "Test Estimate\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "         -1       0.76      0.73      0.75        26\n",
        "          1       0.74      0.77      0.75        26\n",
        "\n",
        "avg / total       0.75      0.75      0.75        52\n",
        "\n",
        "APR: 0.866\n",
        "ROC: 0.846\n",
        "CPU times: user 1min 33s, sys: 3.89 s, total: 1min 37s\n",
        "Wall time: 1min 37s\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note:** as expected the capitalization makes the predicitve task harder since it expands the vocabulary size and adds variations that look random"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Discriminative model on real valued vector labels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets make a vector encoding for the chars simply using a random encoding \n",
      "#and a small amount of noise for the capitalized versions\n",
      "\n",
      "#we can generate a few encodings and let the algorithm choose the best one.\n",
      "encodings, default_encoding = make_encodings(n_encodings, encoding_vector_dimension, sequence_length, noise_size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets define the 3 main machines: 1) pre_processor, 2) vectorizer, 3) estimator\n",
      "\n",
      "#the pre_processor takes the raw format and makes graphs\n",
      "def pre_processor( seqs, encoding=None, default_encoding=None, **args ):\n",
      "    #convert sequences to path graphs\n",
      "    from eden.converter.graph.sequence import sequence_to_eden\n",
      "    graphs = sequence_to_eden(seqs)\n",
      "    \n",
      "    #relabel nodes with corresponding vector encoding\n",
      "    from eden.modifier.graph.vertex_attributes import translate \n",
      "    graphs = translate(graphs, label_map = encoding, default = default_encoding)\n",
      "    \n",
      "    return graphs  \n",
      "\n",
      "#the vectorizer takes graphs and makes sparse vectors\n",
      "from eden.graph import Vectorizer\n",
      "vectorizer = Vectorizer()\n",
      "\n",
      "#the estimator takes a sparse data matrix and a target column vector and makes a predictive model \n",
      "from sklearn.linear_model import SGDClassifier\n",
      "estimator = SGDClassifier(class_weight='auto', shuffle=True)\n",
      "\n",
      "#the model takes a pre_processor, a vectorizer, an estimator and returns the predictive model\n",
      "from eden.model import ActiveLearningBinaryClassificationModel\n",
      "model = ActiveLearningBinaryClassificationModel( pre_processor, estimator=estimator, vectorizer=vectorizer )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets define hyper-parameters vaule ranges\n",
      "from numpy.random import randint\n",
      "from numpy.random import uniform\n",
      "\n",
      "pre_processor_parameters={'encoding':encodings, 'default_encoding':[default_encoding]}\n",
      "\n",
      "vectorizer_parameters={'complexity':[complexity],\n",
      "                       'discretization_size':randint(3, 100, size=n_iter_search), \n",
      "                       'discretization_dimension':randint(3, 50, size=n_iter_search)}\n",
      "\n",
      "estimator_parameters={'n_iter':randint(5, 100, size=n_iter_search),\n",
      "                      'penalty':['l1','l2','elasticnet'],\n",
      "                      'l1_ratio':uniform(0.1,0.9, size=n_iter_search), \n",
      "                      'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
      "                      'power_t':uniform(0.1, size=n_iter_search),\n",
      "                      'alpha': [10**x for x in range(-8,0)],\n",
      "                      'eta0': [10**x for x in range(-4,-1)],\n",
      "                      'learning_rate': [\"invscaling\", \"constant\", \"optimal\"]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Model Auto Optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#optimize hyperparameters and fit a predictive model\n",
      "\n",
      "#determine optimal parameter configuration\n",
      "model.optimize(train_pos_seqs, train_neg_seqs,\n",
      "               model_name='my_seq.model', \n",
      "               fit_vectorizer=True, \n",
      "               n_active_learning_iterations=0,\n",
      "               n_iter=n_iter_search, cv=3, verbose=1,\n",
      "               pre_processor_parameters=pre_processor_parameters, \n",
      "               vectorizer_parameters=vectorizer_parameters, \n",
      "               estimator_parameters=estimator_parameters)\n",
      "\n",
      "#print optimal parameter configuration\n",
      "model.print_model_parameter_configuration()\n",
      "\n",
      "#evaluate predictive performance\n",
      "apr, roc = model.estimate(test_pos_seqs, test_neg_seqs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration: 1/30 (at 11.7 sec; 0:00:11.656160)\n",
        "Best score (roc_auc): 0.710402 (0.842914 +- 0.132512)\n",
        "Instances: 50 ; Features: 1048577 with an avg of 1695 features per instance\n",
        "class: 1 count:25 (0.50)\tclass: -1 count:25 (0.50)\t\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 2/30 (at 22.9 sec; 0:00:22.908120)\n",
        "Best score (roc_auc): 0.729003 (0.781346 +- 0.052343)\n",
        "Instances: 50 ; Features: 1048577 with an avg of 1601 features per instance\n",
        "class: 1 count:25 (0.50)\tclass: -1 count:25 (0.50)\t\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 3/30 (at 45.9 sec; 0:00:45.914586)\n",
        "Best score (roc_auc): 0.758895 (0.860983 +- 0.102087)\n",
        "Instances: 50 ; Features: 1048577 with an avg of 2506 features per instance\n",
        "class: 1 count:25 (0.50)\tclass: -1 count:25 (0.50)\t\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 5/30 (at 58.9 sec; 0:00:58.860697)\n",
        "Best score (roc_auc): 0.971617 (0.984375 +- 0.012758)\n",
        "Instances: 50 ; Features: 1048577 with an avg of 755 features per instance\n",
        "class: 1 count:25 (0.50)\tclass: -1 count:25 (0.50)\t\n",
        "--------------------------------------------------------------------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'default_encoding': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        " 'encoding': {'A': [0.15819304554271105,\n",
        "                    0.8179644501790253,\n",
        "                    0.3976826118350254,\n",
        "                    0.52178941609046137,\n",
        "                    0.5388775220862605,\n",
        "                    0.086024573761096082,\n",
        "                    0.78288082827382,\n",
        "                    0.2149552540245129,\n",
        "                    1.0143453879462339],\n",
        "              'B': [0.11065238308375953,\n",
        "                    0.40736476473166183,\n",
        "                    0.99933658932557434,\n",
        "                    0.34407484254617526,\n",
        "                    0.10406812359919666,\n",
        "                    0.49111541347998738,\n",
        "                    0.39997975509654643,\n",
        "                    0.81951562416135304,\n",
        "                    0.33697217078790082],\n",
        "              'C': [0.25468725777559431,\n",
        "                    0.98411406173291271,\n",
        "                    0.018447456934198581,\n",
        "                    0.21078960691138193,\n",
        "                    0.42773756413850361,\n",
        "                    0.77343251519020528,\n",
        "                    0.54163437576301621,\n",
        "                    0.93054277405662178,\n",
        "                    0.8654729438833656],\n",
        "              'D': [0.15056923565009242,\n",
        "                    0.60235289094916955,\n",
        "                    0.72928485148377453,\n",
        "                    0.25096666030369985,\n",
        "                    0.20194600357213449,\n",
        "                    0.34043954940287458,\n",
        "                    1.0063826321738305,\n",
        "                    0.48094983363181226,\n",
        "                    0.58797309723634616],\n",
        "              'E': [0.26294066939593175,\n",
        "                    0.12299125578456446,\n",
        "                    0.23028682882573082,\n",
        "                    0.63332174564446941,\n",
        "                    0.39203749834275736,\n",
        "                    0.52751530041704942,\n",
        "                    0.96164782433635876,\n",
        "                    0.47607276465156351,\n",
        "                    0.5223618260894346],\n",
        "              'F': [0.19926385336175781,\n",
        "                    0.82280059549489326,\n",
        "                    0.11872941563944599,\n",
        "                    0.20261006090529313,\n",
        "                    0.82877300020020606,\n",
        "                    0.38111819231429633,\n",
        "                    0.85395246650495948,\n",
        "                    0.18419088591757232,\n",
        "                    0.37082756309940623],\n",
        "              'G': [0.40418053252454988,\n",
        "                    0.11013717350570318,\n",
        "                    0.14840501294037159,\n",
        "                    0.19379400772964137,\n",
        "                    0.92357702457501822,\n",
        "                    0.89248271321635197,\n",
        "                    0.39275135735182937,\n",
        "                    0.79809226333228578,\n",
        "                    0.34534257314638678],\n",
        "              'H': [0.94875211089836398,\n",
        "                    0.22555197400355728,\n",
        "                    0.2340353117787165,\n",
        "                    0.4638217739432306,\n",
        "                    0.032979433102274663,\n",
        "                    0.89968395636655385,\n",
        "                    0.77029983040553762,\n",
        "                    0.62869925875424515,\n",
        "                    0.87217269502348194],\n",
        "              'a': [0.1148911195170198,\n",
        "                    0.77898765797670111,\n",
        "                    0.36620411305772238,\n",
        "                    0.4756710952568074,\n",
        "                    0.53226677064959549,\n",
        "                    0.056158416643266262,\n",
        "                    0.77215212316677861,\n",
        "                    0.16701658585726575,\n",
        "                    0.99818451450378509],\n",
        "              'b': [0.086095904097220788,\n",
        "                    0.39013061469965582,\n",
        "                    0.9586937514422893,\n",
        "                    0.31486580775787942,\n",
        "                    0.10098236058786658,\n",
        "                    0.46457205236069887,\n",
        "                    0.35390218877826951,\n",
        "                    0.7946550112420222,\n",
        "                    0.30501071266778301],\n",
        "              'c': [0.20716999668031333,\n",
        "                    0.93442727792493008,\n",
        "                    0.017959809578824038,\n",
        "                    0.17975104875737258,\n",
        "                    0.41415264798023477,\n",
        "                    0.76150506806036666,\n",
        "                    0.51815922773766898,\n",
        "                    0.90291015288482135,\n",
        "                    0.83743740062667971],\n",
        "              'd': [0.10486897292658881,\n",
        "                    0.58900656029981058,\n",
        "                    0.71157155497594438,\n",
        "                    0.20133500334581,\n",
        "                    0.19237508199557918,\n",
        "                    0.29240502258970191,\n",
        "                    0.98541374412127236,\n",
        "                    0.45233484839046856,\n",
        "                    0.57615806711099959],\n",
        "              'e': [0.22091807437785049,\n",
        "                    0.1179341978117121,\n",
        "                    0.19017766803101221,\n",
        "                    0.61642952424778352,\n",
        "                    0.38992833888710643,\n",
        "                    0.49919602331723212,\n",
        "                    0.91470378396754004,\n",
        "                    0.45772487854391208,\n",
        "                    0.50596173895291086],\n",
        "              'f': [0.17318051488894892,\n",
        "                    0.78491589621794855,\n",
        "                    0.077242854672862737,\n",
        "                    0.19609771216989602,\n",
        "                    0.79606822556332768,\n",
        "                    0.37929018189269925,\n",
        "                    0.84351592254383134,\n",
        "                    0.17619888772814252,\n",
        "                    0.32891064637450829],\n",
        "              'g': [0.37736061670240606,\n",
        "                    0.10587898796827266,\n",
        "                    0.12530578530369574,\n",
        "                    0.18084816959572969,\n",
        "                    0.88800710524355075,\n",
        "                    0.87656804698435198,\n",
        "                    0.36675272298544059,\n",
        "                    0.75206563643573221,\n",
        "                    0.3197816065558915],\n",
        "              'h': [0.90152994643721129,\n",
        "                    0.17899580138668836,\n",
        "                    0.23037830477649179,\n",
        "                    0.43881157596215392,\n",
        "                    0.020982545215512238,\n",
        "                    0.87461858467223907,\n",
        "                    0.72062595919798722,\n",
        "                    0.61828681816970887,\n",
        "                    0.86393816373455667]}}\n",
        "Vectorizer:\n",
        "{'complexity': 2, 'discretization_dimension': 16, 'discretization_size': 11}\n",
        "Estimator:\n",
        "{'alpha': 0.0001,\n",
        " 'eta0': 0.0001,\n",
        " 'l1_ratio': 0.8614959728056133,\n",
        " 'learning_rate': 'constant',\n",
        " 'loss': 'log',\n",
        " 'n_iter': 94,\n",
        " 'penalty': 'elasticnet',\n",
        " 'power_t': 0.8093213428263043}\n",
        "--------------------------------------------------------------------------------\n",
        "Classifier:\n",
        "SGDClassifier(alpha=0.0001, class_weight='auto', epsilon=0.1, eta0=0.0001,\n",
        "       fit_intercept=True, l1_ratio=0.8614959728056133,\n",
        "       learning_rate='constant', loss='log', n_iter=94, n_jobs=1,\n",
        "       penalty='elasticnet', power_t=0.8093213428263043, random_state=None,\n",
        "       shuffle=True, verbose=0, warm_start=False)\n",
        "--------------------------------------------------------------------------------\n",
        "Instances: 52 ; Features: 1048577 with an avg of 755 features per instance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------------------------------------------------\n",
        "Test Estimate\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "         -1       0.82      0.88      0.85        26\n",
        "          1       0.88      0.81      0.84        26\n",
        "\n",
        "avg / total       0.85      0.85      0.85        52\n",
        "\n",
        "APR: 0.961\n",
        "ROC: 0.956\n",
        "--------------------------------------------------------------------------------\n",
        "CPU times: user 5min 17s, sys: 2.98 s, total: 5min 20s\n",
        "Wall time: 5min 20s\n"
       ]
      }
     ],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}