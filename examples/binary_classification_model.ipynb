{
 "metadata": {
  "name": "",
  "signature": "sha256:5dda1ba5fcc54debc9d59a5c48f43eb1040892a25c4d5c21271addde984a2626"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rfam_uri(family_id):\n",
      "    return 'http://rfam.xfam.org/family/%s/alignment?acc=%s&format=fastau&download=0'%(family_id,family_id)\n",
      "\n",
      "def rfam_uri(family_id):\n",
      "    return '%s.fa'%(family_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfam_id = 'RF02275' #Hammerhead_HH9\n",
      "rfam_id = 'RF00871' #microRNA mir-689\n",
      "rfam_id = 'RF00005' #tRNA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pre_processor( data, **args):\n",
      "    from eden.converter.rna.rnafold import rnafold_to_eden\n",
      "    graphs = rnafold_to_eden( data, **args )\n",
      "    return graphs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pre_processor( data, **args):\n",
      "    from eden.converter.rna.rnashapes import rnashapes_to_eden\n",
      "    graphs = rnashapes_to_eden( data, **args )\n",
      "    return graphs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from eden.graph import Vectorizer\n",
      "vectorizer = Vectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
      "estimator = PassiveAggressiveClassifier(shuffle=True)\n",
      "estimator = Perceptron(class_weight='auto', shuffle=True)\n",
      "estimator = SGDClassifier(average=True, class_weight='auto', shuffle=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data setup\n",
      "model_fname='eden_model_%s'%rfam_id\n",
      "size=100\n",
      "train_test_split=0.5\n",
      "n_iter=40"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#BinaryClassificationModel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "times=2\n",
      "#create iterable from files\n",
      "from eden.converter.fasta import fasta_to_sequence\n",
      "seqs = fasta_to_sequence( rfam_uri( rfam_id ) )\n",
      "from itertools import tee\n",
      "seqs,seqs_=tee(seqs)\n",
      "iterable_pos = seqs\n",
      "from eden.modifier.seq import seq_to_seq, shuffle_modifier\n",
      "iterable_neg = seq_to_seq( seqs_, modifier=shuffle_modifier, times=times, order=2 )\n",
      "\n",
      "#consier only first 'size' elements\n",
      "from itertools import islice\n",
      "iterable_pos = islice(iterable_pos,size)\n",
      "iterable_neg = islice(iterable_neg,size*times)\n",
      "\n",
      "#split train/test\n",
      "from eden.util import random_bipartition_iter\n",
      "iterable_pos_train, iterable_pos_test = random_bipartition_iter(iterable_pos, relative_size=train_test_split)\n",
      "iterable_neg_train, iterable_neg_test = random_bipartition_iter(iterable_neg, relative_size=train_test_split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#make predictive model\n",
      "from eden.model import ActiveLearningBinaryClassificationModel\n",
      "model = ActiveLearningBinaryClassificationModel(pre_processor=pre_processor, \n",
      "                                                estimator=estimator, \n",
      "                                                vectorizer=vectorizer,\n",
      "                                                n_jobs=1,\n",
      "                                                n_blocks=5)\n",
      "\n",
      "#optimize hyperparameters and fit model\n",
      "from numpy.random import randint\n",
      "from numpy.random import uniform\n",
      "pre_processor_parameters={'max_num':[3], \n",
      "                          'shape_type':[5], \n",
      "                          'energy_range':[30]}\n",
      "\n",
      "vectorizer_parameters={'complexity':[2]}\n",
      "\n",
      "estimator_parameters={'n_iter':randint(5, 100, size=n_iter),\n",
      "                      'penalty':['l1','l2','elasticnet'],\n",
      "                      'l1_ratio':uniform(0.1,0.9, size=n_iter), \n",
      "                      'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
      "                      'power_t':uniform(0.1, size=n_iter),\n",
      "                      'alpha': [10**x for x in range(-8,0)],\n",
      "                      'eta0': [10**x for x in range(-4,-1)],\n",
      "                      'learning_rate': [\"invscaling\", \"constant\", \"optimal\"],\n",
      "                      'n_jobs':[1]}\n",
      "\n",
      "model.optimize(iterable_pos_train, iterable_neg_train, \n",
      "               model_name=model_fname,\n",
      "               max_total_time=60*30, n_iter=n_iter, \n",
      "               cv=5, verbose=2,\n",
      "               score_func=lambda avg_score,std_score : avg_score - std_score * 10,\n",
      "               scoring='roc_auc',\n",
      "               pre_processor_parameters=pre_processor_parameters, \n",
      "               vectorizer_parameters=vectorizer_parameters, \n",
      "               estimator_parameters=estimator_parameters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--------------------------------------------------------------------------------\n",
        "Parameters range:\n",
        "Pre_processor:\n",
        "{'energy_range': [30], 'max_num': [3], 'shape_type': [5]}\n",
        "Vectorizer:\n",
        "{'complexity': [2]}\n",
        "Estimator:\n",
        "{'alpha': [1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1],\n",
        " 'eta0': [0.0001, 0.001, 0.01],\n",
        " 'l1_ratio': array([ 0.63317244,  0.51886851,  0.35268858,  0.76690594,  0.2932573 ,\n",
        "        0.66600557,  0.38869071,  0.31964922,  0.87658431,  0.6682007 ,\n",
        "        0.11415903,  0.43094956,  0.57969777,  0.12878419,  0.83430501,\n",
        "        0.43749819,  0.12135571,  0.4186743 ,  0.68591213,  0.31867484,\n",
        "        0.41876204,  0.4836578 ,  0.67302165,  0.74298608,  0.83815361,\n",
        "        0.4976081 ,  0.77259705,  0.31757236,  0.17725128,  0.19411971,\n",
        "        0.81940094,  0.25818998,  0.26510243,  0.17394977,  0.8044559 ,\n",
        "        0.11019686,  0.22624233,  0.58056084,  0.59424906,  0.20007365]),\n",
        " 'learning_rate': ['invscaling', 'constant', 'optimal'],\n",
        " 'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        " 'n_iter': array([44, 20, 21, 54, 23, 90, 93, 79, 22, 55, 52, 77, 99, 46, 20, 30, 69,\n",
        "       66, 49, 96, 55, 71, 68, 91, 85, 33, 96, 71, 75, 86, 35,  9, 61, 95,\n",
        "       13, 58, 52, 17, 77, 79]),\n",
        " 'n_jobs': [1],\n",
        " 'penalty': ['l1', 'l2', 'elasticnet'],\n",
        " 'power_t': array([ 0.60123389,  0.63295184,  0.50420019,  0.2728489 ,  0.22939716,\n",
        "        0.70168812,  0.39305999,  0.75966011,  0.59248639,  0.32756244,\n",
        "        0.85362276,  0.69479854,  0.17874886,  0.72607391,  0.81444451,\n",
        "        0.35280798,  0.70860853,  0.97504984,  0.19582381,  0.34319167,\n",
        "        0.93150764,  0.55578672,  0.86922492,  0.6212685 ,  0.79485828,\n",
        "        0.5978628 ,  0.92414084,  0.76691539,  0.12000837,  0.66004373,\n",
        "        0.16959527,  0.59337522,  0.29850291,  0.20885484,  0.11375961,\n",
        "        0.8251709 ,  0.77081962,  0.89956492,  0.42058431,  0.49409868])}\n",
        "--------------------------------------------------------------------------------\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 4/40 (at 44.2 sec; 0:00:44.227759)\n",
        "Best score (roc_auc): 0.285667 (0.886000 +- 0.060033)\n",
        "Instances: 300 ; Features: 1048577 with an avg of 923 features per instance\n",
        "class: 1 count:100 (0.33)\tclass: -1 count:200 (0.67)\t\n",
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 1e-05,\n",
        " 'eta0': 0.0001,\n",
        " 'l1_ratio': 0.49760810133542532,\n",
        " 'learning_rate': 'invscaling',\n",
        " 'loss': 'modified_huber',\n",
        " 'n_iter': 22,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'l1',\n",
        " 'power_t': 0.69479853766137778}\n",
        "--------------------------------------------------------------------------------\n",
        "Saved current best model in eden_model_RF00005\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 5/40 (at 45.5 sec; 0:00:45.498491)\n",
        "Best score (roc_auc): 0.519811 (0.941000 +- 0.042119)\n",
        "Instances: 300 ; Features: 1048577 with an avg of 923 features per instance\n",
        "class: 1 count:100 (0.33)\tclass: -1 count:200 (0.67)\t\n",
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 1e-08,\n",
        " 'eta0': 0.01,\n",
        " 'l1_ratio': 0.12135571028113326,\n",
        " 'learning_rate': 'optimal',\n",
        " 'loss': 'perceptron',\n",
        " 'n_iter': 13,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'elasticnet',\n",
        " 'power_t': 0.72607390984530573}\n",
        "--------------------------------------------------------------------------------\n",
        "Saved current best model in eden_model_RF00005\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 7/40 (at 56.8 sec; 0:00:56.832274)\n",
        "Best score (roc_auc): 0.532548 (0.948000 +- 0.041545)\n",
        "Instances: 300 ; Features: 1048577 with an avg of 923 features per instance\n",
        "class: 1 count:100 (0.33)\tclass: -1 count:200 (0.67)\t\n",
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 1e-07,\n",
        " 'eta0': 0.01,\n",
        " 'l1_ratio': 0.41867430407886619,\n",
        " 'learning_rate': 'constant',\n",
        " 'loss': 'log',\n",
        " 'n_iter': 69,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'elasticnet',\n",
        " 'power_t': 0.55578671556918213}\n",
        "--------------------------------------------------------------------------------\n",
        "Saved current best model in eden_model_RF00005\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 9/40 (at 63.6 sec; 0:01:03.569072)\n",
        "Best score (roc_auc): 0.633552 (0.945000 +- 0.031145)\n",
        "Instances: 300 ; Features: 1048577 with an avg of 923 features per instance\n",
        "class: 1 count:100 (0.33)\tclass: -1 count:200 (0.67)\t\n",
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 0.0001,\n",
        " 'eta0': 0.01,\n",
        " 'l1_ratio': 0.66820070314950597,\n",
        " 'learning_rate': 'constant',\n",
        " 'loss': 'perceptron',\n",
        " 'n_iter': 93,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'l2',\n",
        " 'power_t': 0.49409867817817121}\n",
        "--------------------------------------------------------------------------------\n",
        "Saved current best model in eden_model_RF00005\n",
        "--------------------------------------------------------------------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parameters range:\n",
        "Pre_processor:\n",
        "{'energy_range': [30, 30, 30, 30],\n",
        " 'max_num': [3, 3, 3, 3],\n",
        " 'shape_type': [5, 5, 5, 5]}\n",
        "Vectorizer:\n",
        "{'complexity': [2, 2, 2, 2]}\n",
        "Estimator:\n",
        "{'alpha': [1e-05, 1e-08, 1e-07, 0.0001],\n",
        " 'eta0': [0.0001, 0.01, 0.01, 0.01],\n",
        " 'l1_ratio': [0.49760810133542532,\n",
        "              0.12135571028113326,\n",
        "              0.41867430407886619,\n",
        "              0.66820070314950597],\n",
        " 'learning_rate': ['invscaling', 'optimal', 'constant', 'constant'],\n",
        " 'loss': ['modified_huber', 'perceptron', 'log', 'perceptron'],\n",
        " 'n_iter': [22, 13, 69, 93],\n",
        " 'n_jobs': [1, 1, 1, 1],\n",
        " 'penalty': ['l1', 'elasticnet', 'elasticnet', 'l2'],\n",
        " 'power_t': [0.69479853766137778,\n",
        "             0.72607390984530573,\n",
        "             0.55578671556918213,\n",
        "             0.49409867817817121]}\n",
        "--------------------------------------------------------------------------------\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 27/40 (at 114.4 sec; 0:01:54.395948)\n",
        "Best score (roc_auc): 0.735092 (0.973000 +- 0.023791)\n",
        "Instances: 300 ; Features: 1048577 with an avg of 923 features per instance\n",
        "class: 1 count:100 (0.33)\tclass: -1 count:200 (0.67)\t\n",
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 1e-07,\n",
        " 'eta0': 0.01,\n",
        " 'l1_ratio': 0.12135571028113326,\n",
        " 'learning_rate': 'constant',\n",
        " 'loss': 'perceptron',\n",
        " 'n_iter': 22,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'l2',\n",
        " 'power_t': 0.72607390984530573}\n",
        "--------------------------------------------------------------------------------\n",
        "Saved current best model in eden_model_RF00005\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration: 35/40 (at 123.4 sec; 0:02:03.394260)\n",
        "Best score (roc_auc): 0.778800 (0.956000 +- 0.017720)\n",
        "Instances: 300 ; Features: 1048577 with an avg of 923 features per instance\n",
        "class: 1 count:100 (0.33)\tclass: -1 count:200 (0.67)\t\n",
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 1e-07,\n",
        " 'eta0': 0.01,\n",
        " 'l1_ratio': 0.12135571028113326,\n",
        " 'learning_rate': 'optimal',\n",
        " 'loss': 'perceptron',\n",
        " 'n_iter': 13,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'elasticnet',\n",
        " 'power_t': 0.55578671556918213}\n",
        "--------------------------------------------------------------------------------\n",
        "Saved current best model in eden_model_RF00005\n",
        "CPU times: user 2min 2s, sys: 3.8 s, total: 2min 6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2min 10s\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#estimate predictive performance\n",
      "model.print_model_parameter_configuration()\n",
      "model.estimate( iterable_pos_test, iterable_neg_test )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--------------------------------------------------------------------------------\n",
        "Current parameters:\n",
        "Pre_processor:\n",
        "{'energy_range': 30, 'max_num': 3, 'shape_type': 5}\n",
        "Vectorizer:\n",
        "{'complexity': 2}\n",
        "Estimator:\n",
        "{'alpha': 1e-07,\n",
        " 'eta0': 0.01,\n",
        " 'l1_ratio': 0.12135571028113326,\n",
        " 'learning_rate': 'optimal',\n",
        " 'loss': 'perceptron',\n",
        " 'n_iter': 13,\n",
        " 'n_jobs': 1,\n",
        " 'penalty': 'elasticnet',\n",
        " 'power_t': 0.55578671556918213}\n",
        "--------------------------------------------------------------------------------\n",
        "Classifier:\n",
        "SGDClassifier(alpha=1e-07, average=True, class_weight='auto', epsilon=0.1,\n",
        "       eta0=0.01, fit_intercept=True, l1_ratio=0.12135571028113326,\n",
        "       learning_rate='optimal', loss='perceptron', n_iter=13, n_jobs=1,\n",
        "       penalty='elasticnet', power_t=0.55578671556918213,\n",
        "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
        "--------------------------------------------------------------------------------\n",
        "Instances: 300 ; Features: 1048577 with an avg of 863 features per instance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------------------------------------------------\n",
        "Test Estimate\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "         -1       0.95      0.91      0.93       200\n",
        "          1       0.83      0.90      0.87       100\n",
        "\n",
        "avg / total       0.91      0.91      0.91       300\n",
        "\n",
        "APR: 0.961\n",
        "ROC: 0.976\n",
        "--------------------------------------------------------------------------------\n",
        "CPU times: user 24 s, sys: 1.11 s, total: 25.1 s\n",
        "Wall time: 28.5 s\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Models can be reloaded from disk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from eden.model import ActiveLearningBinaryClassificationModel\n",
      "\n",
      "model2 = ActiveLearningBinaryClassificationModel()\n",
      "model2.load(model_fname)\n",
      "\n",
      "from eden.converter.fasta import fasta_to_sequence\n",
      "seqs = fasta_to_sequence( rfam_uri( rfam_id ) )\n",
      "from itertools import tee\n",
      "seqs,seqs_=tee(seqs)\n",
      "iterable_pos = seqs\n",
      "\n",
      "#consier only first 'size' elements\n",
      "from itertools import islice\n",
      "iterable_pos = islice(iterable_pos,size)\n",
      "\n",
      "predictions= model2.decision_function( iterable_pos )\n",
      "for n,i in enumerate(sorted(predictions)): print n,i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 -0.730557492715\n",
        "1 -0.730557492715\n",
        "2 -0.589518526906\n",
        "3 -0.589518526906\n",
        "4 -0.528131844396\n",
        "5 -0.528131844396\n",
        "6 -0.443252117698\n",
        "7 -0.443252117698\n",
        "8 -0.412102450671\n",
        "9 -0.412102450671\n",
        "10 -0.317745863651\n",
        "11 -0.317745863651\n",
        "12 -0.291657023121\n",
        "13 -0.291657023121\n",
        "14 -0.280506178774\n",
        "15 -0.280506178774\n",
        "16 -0.271073357919\n",
        "17 -0.271073357919\n",
        "18 -0.267545940445\n",
        "19 -0.267545940445\n",
        "20 -0.223803195176\n",
        "21 -0.223803195176\n",
        "22 -0.211402698918\n",
        "23 -0.211402698918\n",
        "24 -0.197259330646\n",
        "25 -0.197259330646\n",
        "26 -0.189486912345\n",
        "27 -0.189486912345\n",
        "28 -0.189353063161\n",
        "29 -0.189353063161\n",
        "30 -0.186147191128\n",
        "31 -0.186147191128\n",
        "32 -0.176033349987\n",
        "33 -0.176033349987\n",
        "34 -0.173169102264\n",
        "35 -0.173169102264\n",
        "36 -0.150424194824\n",
        "37 -0.150424194824\n",
        "38 -0.126970271583\n",
        "39 -0.126970271583\n",
        "40 -0.126850886972\n",
        "41 -0.126850886972\n",
        "42 -0.120937875275\n",
        "43 -0.120937875275\n",
        "44 -0.120140972815\n",
        "45 -0.120140972815\n",
        "46 -0.0945749063812\n",
        "47 -0.0945749063812\n",
        "48 -0.0840720003107\n",
        "49 -0.0840720003107\n",
        "50 -0.0781692055208\n",
        "51 -0.0781692055208\n",
        "52 -0.0751131604936\n",
        "53 -0.0751131604936\n",
        "54 -0.0257005044\n",
        "55 -0.0257005044\n",
        "56 -0.0194107343371\n",
        "57 -0.0194107343371\n",
        "58 -0.0191036259772\n",
        "59 -0.0191036259772\n",
        "60 -0.0181068686914\n",
        "61 -0.0181068686914\n",
        "62 -0.015819767169\n",
        "63 -0.015819767169\n",
        "64 -0.0153600608137\n",
        "65 -0.0153600608137\n",
        "66 0.00164536625934\n",
        "67 0.00164536625934\n",
        "68 0.00284444282437\n",
        "69 0.00284444282437\n",
        "70 0.0079898298846\n",
        "71 0.0079898298846\n",
        "72 0.0112760224904\n",
        "73 0.0112760224904\n",
        "74 0.0114947045069\n",
        "75 0.0114947045069\n",
        "76 0.0380396275016\n",
        "77 0.0380396275016\n",
        "78 0.0431847905793\n",
        "79 0.0431847905793\n",
        "80 0.0531465765359\n",
        "81 0.0531465765359\n",
        "82 0.0739606925638\n",
        "83 0.0739606925638\n",
        "84 0.0779807818579\n",
        "85 0.0779807818579\n",
        "86 0.10055615217\n",
        "87 0.10055615217\n",
        "88 0.114417412952\n",
        "89 0.114417412952\n",
        "90 0.131220436399\n",
        "91 0.131220436399\n",
        "92 0.144177598078\n",
        "93 0.144177598078\n",
        "94 0.145758958046\n",
        "95 0.145758958046\n",
        "96 0.152643487045\n",
        "97 0.152643487045\n",
        "98 0.157363755008\n",
        "99 0.157363755008\n",
        "100 0.184941344525\n",
        "101 0.184941344525\n",
        "102 0.196857766022\n",
        "103 0.196857766022\n",
        "104 0.21466332507\n",
        "105 0.21466332507\n",
        "106 0.22422448603\n",
        "107 0.22422448603\n",
        "108 0.238690029953\n",
        "109 0.238690029953\n",
        "110 0.242736661082\n",
        "111 0.242736661082\n",
        "112 0.263799142193\n",
        "113 0.263799142193\n",
        "114 0.269957498325\n",
        "115 0.269957498325\n",
        "116 0.272067935193\n",
        "117 0.272067935193\n",
        "118 0.276822939394\n",
        "119 0.276822939394\n",
        "120 0.279891309664\n",
        "121 0.279891309664\n",
        "122 0.326490834454\n",
        "123 0.326490834454\n",
        "124 0.326951489786\n",
        "125 0.326951489786\n",
        "126 0.33160211652\n",
        "127 0.33160211652\n",
        "128 0.338826823971\n",
        "129 0.338826823971\n",
        "130 0.342144040022\n",
        "131 0.342144040022\n",
        "132 0.34215068147\n",
        "133 0.34215068147\n",
        "134 0.349343650338\n",
        "135 0.349343650338\n",
        "136 0.35023116649\n",
        "137 0.35023116649\n",
        "138 0.353439308497\n",
        "139 0.353439308497\n",
        "140 0.391297377185\n",
        "141 0.391297377185\n",
        "142 0.405881557462\n",
        "143 0.405881557462\n",
        "144 0.408734628973\n",
        "145 0.408734628973\n",
        "146 0.413157192821\n",
        "147 0.413157192821\n",
        "148 0.425600749322\n",
        "149 0.425600749322\n",
        "150 0.427026049957\n",
        "151 0.427026049957\n",
        "152 0.440552277268\n",
        "153 0.440552277268\n",
        "154 0.44445847225\n",
        "155 0.44445847225\n",
        "156 0.445509878732\n",
        "157 0.445509878732\n",
        "158 0.447556716557\n",
        "159 0.447556716557\n",
        "160 0.45038768687\n",
        "161 0.45038768687\n",
        "162 0.453209517716\n",
        "163 0.453209517716\n",
        "164 0.462487651019\n",
        "165 0.462487651019\n",
        "166 0.48286991041\n",
        "167 0.48286991041\n",
        "168 0.484794366148\n",
        "169 0.484794366148\n",
        "170 0.489186610898\n",
        "171 0.489186610898\n",
        "172 0.49073195685\n",
        "173 0.49073195685\n",
        "174 0.492781880708\n",
        "175 0.492781880708\n",
        "176 0.49806336847\n",
        "177 0.49806336847\n",
        "178 0.501061468241\n",
        "179 0.501061468241\n",
        "180 0.506989119682\n",
        "181 0.506989119682\n",
        "182 0.509438213732\n",
        "183 0.509438213732\n",
        "184 0.516916433557\n",
        "185 0.516916433557\n",
        "186 0.523705946185\n",
        "187 0.523705946185\n",
        "188 0.525707041556\n",
        "189 0.525707041556\n",
        "190 0.532401435706\n",
        "191 0.532401435706\n",
        "192 0.537059288814\n",
        "193 0.537059288814\n",
        "194 0.538792727611\n",
        "195 0.538792727611\n",
        "196 0.539963551902\n",
        "197 0.539963551902\n",
        "198 0.544571380741\n",
        "199 0.544571380741\n",
        "200 0.544734890792\n",
        "201 0.544734890792\n",
        "202 0.552323718523\n",
        "203 0.552323718523\n",
        "204 0.555490269657\n",
        "205 0.555490269657\n",
        "206 0.556830666926\n",
        "207 0.556830666926\n",
        "208 0.558933641135\n",
        "209 0.558933641135\n",
        "210 0.560255260997\n",
        "211 0.560255260997\n",
        "212 0.566693052932\n",
        "213 0.566693052932\n",
        "214 0.575308267391\n",
        "215 0.575308267391\n",
        "216 0.580447193633\n",
        "217 0.580447193633\n",
        "218 0.583091787446\n",
        "219 0.583091787446\n",
        "220 0.583522448748\n",
        "221 0.583522448748\n",
        "222 0.583580135409\n",
        "223 0.583580135409\n",
        "224 0.583706026958\n",
        "225 0.583706026958\n",
        "226 0.587728287286\n",
        "227 0.587728287286\n",
        "228 0.588106526914\n",
        "229 0.588106526914\n",
        "230 0.589088498477\n",
        "231 0.589088498477\n",
        "232 0.602495237852\n",
        "233 0.602495237852\n",
        "234 0.605681229614\n",
        "235 0.605681229614\n",
        "236 0.607696489746\n",
        "237 0.607696489746\n",
        "238 0.611684896245\n",
        "239 0.611684896245\n",
        "240 0.612058793191\n",
        "241 0.612058793191\n",
        "242 0.612944825081\n",
        "243 0.612944825081\n",
        "244 0.616885902649\n",
        "245 0.616885902649\n",
        "246 0.617864459623\n",
        "247 0.617864459623\n",
        "248 0.62053246938\n",
        "249 0.62053246938\n",
        "250 0.622936614746\n",
        "251 0.622936614746\n",
        "252 0.629408854557\n",
        "253 0.629408854557\n",
        "254 0.634368718919\n",
        "255 0.634368718919\n",
        "256 0.636824311784\n",
        "257 0.636824311784\n",
        "258 0.638609645242\n",
        "259 0.638609645242\n",
        "260 0.639225070423\n",
        "261 0.639225070423\n",
        "262 0.640013911625\n",
        "263 0.640013911625\n",
        "264 0.64118276728\n",
        "265 0.64118276728\n",
        "266 0.641977931338\n",
        "267 0.641977931338\n",
        "268 0.649511724272\n",
        "269 0.649511724272\n",
        "270 0.650577393465\n",
        "271 0.650577393465\n",
        "272 0.651051029355\n",
        "273 0.651051029355\n",
        "274 0.651369548592\n",
        "275 0.651369548592\n",
        "276 0.652329709148\n",
        "277 0.652329709148\n",
        "278 0.65899753811\n",
        "279 0.65899753811\n",
        "280 0.661829397671\n",
        "281 0.661829397671\n",
        "282 0.663041383111\n",
        "283 0.663041383111\n",
        "284 0.6632418477\n",
        "285 0.6632418477\n",
        "286 0.664412626605\n",
        "287 0.664412626605\n",
        "288 0.666068204919\n",
        "289 0.666068204919\n",
        "290 0.666306914579\n",
        "291 0.666306914579\n",
        "292 0.666348355066\n",
        "293 0.666348355066\n",
        "294 0.669594003788\n",
        "295 0.669594003788\n",
        "296 0.671161071388\n",
        "297 0.671161071388\n",
        "298 0.671543087554\n",
        "299 0.671543087554\n",
        "300 0.67267252009\n",
        "301 0.67267252009\n",
        "302 0.673974060531\n",
        "303 0.673974060531\n",
        "304 0.673990571138\n",
        "305 0.673990571138\n",
        "306 0.674297558135\n",
        "307 0.674297558135\n",
        "308 0.675285319024\n",
        "309 0.675285319024\n",
        "310 0.682073316953\n",
        "311 0.682073316953\n",
        "312 0.683892399805\n",
        "313 0.683892399805\n",
        "314 0.684010013735\n",
        "315 0.684010013735\n",
        "316 0.686813720354\n",
        "317 0.686813720354\n",
        "318 0.68770604505\n",
        "319 0.68770604505\n",
        "320 0.691932590999\n",
        "321 0.691932590999\n",
        "322 0.701646601022\n",
        "323 0.701646601022\n",
        "324 0.710130522605\n",
        "325 0.710130522605\n",
        "326 0.71811316155\n",
        "327 0.71811316155\n",
        "328 0.720325257344\n",
        "329 0.720325257344\n",
        "330 0.722377816858\n",
        "331 0.722377816858\n",
        "332 0.723514913581\n",
        "333 0.723514913581\n",
        "334 0.726092089963\n",
        "335 0.726092089963\n",
        "336 0.727716975911\n",
        "337 0.727716975911\n",
        "338 0.727985729304\n",
        "339 0.727985729304\n",
        "340 0.731358897018\n",
        "341 0.731358897018\n",
        "342 0.732685900958\n",
        "343 0.732685900958\n",
        "344 0.733340267782\n",
        "345 0.733340267782\n",
        "346 0.734906036135\n",
        "347 0.734906036135\n",
        "348 0.737065614217\n",
        "349 0.737065614217\n",
        "350 0.749762094003\n",
        "351 0.749762094003\n",
        "352 0.752783604859\n",
        "353 0.752783604859\n",
        "354 0.756177523116\n",
        "355 0.756177523116\n",
        "356 0.756746345963\n",
        "357 0.756746345963\n",
        "358 0.758756388108\n",
        "359 0.758756388108\n",
        "360 0.763355585272\n",
        "361 0.763355585272\n",
        "362 0.764107203215\n",
        "363 0.764107203215\n",
        "364 0.772251507512\n",
        "365 0.772251507512\n",
        "366 0.774932693443\n",
        "367 0.774932693443\n",
        "368 0.780646896466\n",
        "369 0.780646896466\n",
        "370 0.780774239892\n",
        "371 0.780774239892\n",
        "372 0.781903010646\n",
        "373 0.781903010646\n",
        "374 0.784292196014\n",
        "375 0.784292196014\n",
        "376 0.784456771383\n",
        "377 0.784456771383\n",
        "378 0.789657143756\n",
        "379 0.789657143756\n",
        "380 0.792890328211\n",
        "381 0.792890328211\n",
        "382 0.794475710149\n",
        "383 0.794475710149\n",
        "384 0.794863495471\n",
        "385 0.794863495471\n",
        "386 0.800052815\n",
        "387 0.800052815\n",
        "388 0.801050930374\n",
        "389 0.801050930374\n",
        "390 0.803059391373\n",
        "391 0.803059391373\n",
        "392 0.804374674524\n",
        "393 0.804374674524\n",
        "394 0.806426140485\n",
        "395 0.806426140485\n",
        "396 0.807406521062\n",
        "397 0.807406521062\n",
        "398 0.807972798252\n",
        "399 0.807972798252\n",
        "400 0.808571149791\n",
        "401 0.808571149791\n",
        "402 0.808622538334\n",
        "403 0.808622538334\n",
        "404 0.808834510085\n",
        "405 0.808834510085\n",
        "406 0.809996883191\n",
        "407 0.809996883191\n",
        "408 0.810286960231\n",
        "409 0.810286960231\n",
        "410 0.810523645135\n",
        "411 0.810523645135\n",
        "412 0.811182628534\n",
        "413 0.811182628534\n",
        "414 0.81288495541\n",
        "415 0.81288495541\n",
        "416 0.818078309141\n",
        "417 0.818078309141\n",
        "418 0.819490320952\n",
        "419 0.819490320952\n",
        "420 0.820766261379\n",
        "421 0.820766261379\n",
        "422 0.821421730206\n",
        "423 0.821421730206\n",
        "424 0.822049872656\n",
        "425 0.822049872656\n",
        "426 0.823504245183\n",
        "427 0.823504245183\n",
        "428 0.824231125512\n",
        "429 0.824231125512\n",
        "430 0.827350221811\n",
        "431 0.827350221811\n",
        "432 0.828087155735\n",
        "433 0.828087155735\n",
        "434 0.833070441837\n",
        "435 0.833070441837\n",
        "436 0.83762388923\n",
        "437 0.83762388923\n",
        "438 0.839835255937\n",
        "439 0.839835255937\n",
        "440 0.84044030722\n",
        "441 0.84044030722\n",
        "442 0.841013610065\n",
        "443 0.841013610065\n",
        "444 0.843074461987\n",
        "445 0.843074461987\n",
        "446 0.843968141783\n",
        "447 0.843968141783\n",
        "448 0.846232842955\n",
        "449 0.846232842955\n",
        "450 0.846957649032\n",
        "451 0.846957649032\n",
        "452 0.847862195813\n",
        "453 0.847862195813\n",
        "454 0.848149838549\n",
        "455 0.848149838549\n",
        "456 0.850345360214\n",
        "457 0.850345360214\n",
        "458 0.850682468909\n",
        "459 0.850682468909\n",
        "460 0.8555941844\n",
        "461 0.8555941844\n",
        "462 0.856600730024\n",
        "463 0.856600730024\n",
        "464 0.857956053615\n",
        "465 0.857956053615\n",
        "466 0.860539772791\n",
        "467 0.860539772791\n",
        "468 0.86075511288\n",
        "469 0.86075511288\n",
        "470 0.863296335012\n",
        "471 0.863296335012\n",
        "472 0.865175308346\n",
        "473 0.865175308346\n",
        "474 0.866075168608\n",
        "475 0.866075168608\n",
        "476 0.869838775369\n",
        "477 0.869838775369\n",
        "478 0.875047960373\n",
        "479 0.875047960373\n",
        "480 0.879740424566\n",
        "481 0.879740424566\n",
        "482 0.880031584151\n",
        "483 0.880031584151\n",
        "484 0.883231970594\n",
        "485 0.883231970594\n",
        "486 0.884040939044\n",
        "487 0.884040939044\n",
        "488 0.884271090287\n",
        "489 0.884271090287\n",
        "490 0.886757979825\n",
        "491 0.886757979825\n",
        "492 0.887007868974\n",
        "493 0.887007868974\n",
        "494 0.888429255278\n",
        "495 0.888429255278\n",
        "496 0.894618966068\n",
        "497 0.894618966068\n",
        "498 0.896105578394\n",
        "499 0.896105578394\n",
        "500 0.897295677793\n",
        "501 0.897295677793\n",
        "502 0.897362545549\n",
        "503 0.897362545549\n",
        "504 0.904001897478\n",
        "505 0.904001897478\n",
        "506 0.905562524153\n",
        "507 0.905562524153\n",
        "508 0.906804168422\n",
        "509 0.906804168422\n",
        "510 0.907997980179\n",
        "511 0.907997980179\n",
        "512 0.909099741432\n",
        "513 0.909099741432\n",
        "514 0.909702405275\n",
        "515 0.909702405275\n",
        "516 0.910867813775\n",
        "517 0.910867813775\n",
        "518 0.91100517339\n",
        "519 0.91100517339\n",
        "520 0.916010076979\n",
        "521 0.916010076979\n",
        "522 0.92236531472\n",
        "523 0.92236531472\n",
        "524 0.922449859637\n",
        "525 0.922449859637\n",
        "526 0.925804567568\n",
        "527 0.925804567568\n",
        "528 0.927252738888\n",
        "529 0.927252738888\n",
        "530 0.928566555448\n",
        "531 0.928566555448\n",
        "532 0.928571447417\n",
        "533 0.928571447417\n",
        "534 0.928982982637\n",
        "535 0.928982982637\n",
        "536 0.931930916157\n",
        "537 0.931930916157\n",
        "538 0.935997933061\n",
        "539 0.935997933061\n",
        "540 0.943452125575\n",
        "541 0.943452125575\n",
        "542 0.946726034948\n",
        "543 0.946726034948\n",
        "544 0.949906150429\n",
        "545 0.949906150429\n",
        "546 0.950375047078\n",
        "547 0.950375047078\n",
        "548 0.952976363815\n",
        "549 0.952976363815\n",
        "550 0.954609435819\n",
        "551 0.954609435819\n",
        "552 0.959654672583\n",
        "553 0.959654672583\n",
        "554 0.966247069443\n",
        "555 0.966247069443\n",
        "556 0.971328500748\n",
        "557 0.971328500748\n",
        "558 0.973359409664\n",
        "559 0.973359409664\n",
        "560 0.975959445118\n",
        "561 0.975959445118\n",
        "562 0.980683022947\n",
        "563 0.980683022947\n",
        "564 0.982305355672\n",
        "565 0.982305355672\n",
        "566 0.984826572569\n",
        "567 0.984826572569\n",
        "568 0.985155091987\n",
        "569 0.985155091987\n",
        "570 0.986555496618\n",
        "571 0.986555496618\n",
        "572 0.987787268105\n",
        "573 0.987787268105\n",
        "574 0.992574355305\n",
        "575 0.992574355305\n",
        "576 0.995925337118\n",
        "577 0.995925337118\n",
        "578 1.00413148172\n",
        "579 1.00413148172\n",
        "580 1.00692130074\n",
        "581 1.00692130074\n",
        "582 1.00889444615\n",
        "583 1.00889444615\n",
        "584 1.01198742116\n",
        "585 1.01198742116\n",
        "586 1.02296766541\n",
        "587 1.02296766541\n",
        "588 1.02535546722\n",
        "589 1.02535546722\n",
        "590 1.0296846266\n",
        "591 1.0296846266\n",
        "592 1.03013054675\n",
        "593 1.03013054675\n",
        "594 1.03187508101\n",
        "595 1.03187508101\n",
        "596 1.03260985984\n",
        "597 1.03260985984\n",
        "598 1.03347103892\n",
        "599 1.03347103892\n",
        "600 1.03521406648\n",
        "601 1.03521406648\n",
        "602 1.0425605375\n",
        "603 1.0425605375\n",
        "604 1.04417309491\n",
        "605 1.04417309491\n",
        "606 1.04903357659\n",
        "607 1.04903357659\n",
        "608 1.05657915988\n",
        "609 1.05657915988\n",
        "610 1.05710536952\n",
        "611 1.05710536952\n",
        "612 1.05966996457\n",
        "613 1.05966996457\n",
        "614 1.06137036696\n",
        "615 1.06137036696\n",
        "616 1.06199406993\n",
        "617 1.06199406993\n",
        "618 1.06593092792\n",
        "619 1.06593092792\n",
        "620 1.0740681788\n",
        "621 1.0740681788\n",
        "622 1.08085836375\n",
        "623 1.08085836375\n",
        "624 1.08250651717\n",
        "625 1.08250651717\n",
        "626 1.08307837143\n",
        "627 1.08307837143\n",
        "628 1.08952897041\n",
        "629 1.08952897041\n",
        "630 1.08962252133\n",
        "631 1.08962252133\n",
        "632 1.09079778037\n",
        "633 1.09079778037\n",
        "634 1.09392727814\n",
        "635 1.09392727814\n",
        "636 1.09932638755\n",
        "637 1.09932638755\n",
        "638 1.10522414692\n",
        "639 1.10522414692\n",
        "640 1.10553921855\n",
        "641 1.10553921855\n",
        "642 1.10900755648\n",
        "643 1.10900755648\n",
        "644 1.11648908015\n",
        "645 1.11648908015\n",
        "646 1.11712190863\n",
        "647 1.11712190863\n",
        "648 1.12066018119\n",
        "649 1.12066018119\n",
        "650 1.12139741859\n",
        "651 1.12139741859\n",
        "652 1.12409886205\n",
        "653 1.12409886205\n",
        "654 1.13339894789\n",
        "655 1.13339894789\n",
        "656 1.13445852866\n",
        "657 1.13445852866\n",
        "658 1.13530129061\n",
        "659 1.13530129061\n",
        "660 1.13707894707\n",
        "661 1.13707894707\n",
        "662 1.1415750165\n",
        "663 1.1415750165\n",
        "664 1.15780902289\n",
        "665 1.15780902289\n",
        "666 1.15816752575\n",
        "667 1.15816752575\n",
        "668 1.16163478935\n",
        "669 1.16163478935\n",
        "670 1.17122732783\n",
        "671 1.17122732783\n",
        "672 1.17569356071\n",
        "673 1.17569356071\n",
        "674 1.17660271486\n",
        "675 1.17660271486\n",
        "676 1.17812607265\n",
        "677 1.17812607265\n",
        "678 1.17890554394\n",
        "679 1.17890554394\n",
        "680 1.18233684397\n",
        "681 1.18233684397\n",
        "682 1.18664314374\n",
        "683 1.18664314374\n",
        "684 1.18673413302\n",
        "685 1.18673413302\n",
        "686 1.19139265739\n",
        "687 1.19139265739\n",
        "688 1.19174025424\n",
        "689 1.19174025424\n",
        "690 1.19358962698\n",
        "691 1.19358962698\n",
        "692 1.19448674308\n",
        "693 1.19448674308\n",
        "694 1.21167202401\n",
        "695 1.21167202401\n",
        "696 1.2171764021\n",
        "697 1.2171764021\n",
        "698 1.21811730354\n",
        "699 1.21811730354\n",
        "700 1.22074351626\n",
        "701 1.22074351626\n",
        "702 1.22238900285\n",
        "703 1.22238900285\n",
        "704 1.22625705221\n",
        "705 1.22625705221\n",
        "706 1.24768933298\n",
        "707 1.24768933298\n",
        "708 1.25430879827\n",
        "709 1.25430879827\n",
        "710 1.25694172786\n",
        "711 1.25694172786\n",
        "712 1.26206138681\n",
        "713 1.26206138681\n",
        "714 1.27043973154\n",
        "715 1.27043973154\n",
        "716 1.27108997982\n",
        "717 1.27108997982\n",
        "718 1.27377367599\n",
        "719 1.27377367599\n",
        "720 1.27569152307\n",
        "721 1.27569152307\n",
        "722 1.28578100546\n",
        "723 1.28578100546\n",
        "724 1.30005180103\n",
        "725 1.30005180103\n",
        "726 1.30145648428\n",
        "727 1.30145648428\n",
        "728 1.32068653273\n",
        "729 1.32068653273\n",
        "730 1.33493860187\n",
        "731 1.33493860187\n",
        "732 1.36311479819\n",
        "733 1.36311479819\n",
        "734 1.36810054168\n",
        "735 1.36810054168\n",
        "736 1.37063892125\n",
        "737 1.37063892125\n",
        "738 1.37462896541\n",
        "739 1.37462896541\n",
        "740 1.38440267483\n",
        "741 1.38440267483\n",
        "742 1.39237812872\n",
        "743 1.39237812872\n",
        "744 1.41222925626\n",
        "745 1.41222925626\n",
        "746 1.42744814221\n",
        "747 1.42744814221\n",
        "748 1.42748552232\n",
        "749 1.42748552232\n",
        "750 1.44834444943\n",
        "751 1.44834444943\n",
        "752 1.45361445889\n",
        "753 1.45361445889\n",
        "754 1.45741828826\n",
        "755 1.45741828826\n",
        "756 1.46397769109\n",
        "757 1.46397769109\n",
        "758 1.50531126415\n",
        "759 1.50531126415\n",
        "760 1.50610949392\n",
        "761 1.50610949392\n",
        "762 1.54069189908\n",
        "763 1.54069189908\n",
        "764 1.57591541632\n",
        "765 1.57591541632\n",
        "766 1.57711272945\n",
        "767 1.57711272945\n",
        "768 1.59026903192\n",
        "769 1.59026903192\n",
        "770 1.60466632579\n",
        "771 1.60466632579\n",
        "772 1.61084932304\n",
        "773 1.61084932304\n",
        "774 1.61894138782\n",
        "775 1.61894138782\n",
        "776 1.63400581063\n",
        "777 1.63400581063\n",
        "778 1.63432016152\n",
        "779 1.63432016152\n",
        "780 1.64068033725\n",
        "781 1.64068033725\n",
        "782 1.65212283882\n",
        "783 1.65212283882\n",
        "784 1.65571633912\n",
        "785 1.65571633912\n",
        "786 1.74749818621\n",
        "787 1.74749818621\n",
        "788 1.75038648564\n",
        "789 1.75038648564\n",
        "790 1.7668220754\n",
        "791 1.7668220754\n",
        "792 1.80054839683\n",
        "793 1.80054839683\n",
        "794 1.80068983175\n",
        "795 1.80068983175\n",
        "796 1.81238468534\n",
        "797 1.81238468534\n",
        "798 1.89408262098\n",
        "799 1.89408262098\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#ActiveLearningBinaryClassificationModel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "times=5\n",
      "#create iterable from files\n",
      "from eden.converter.fasta import fasta_to_sequence\n",
      "seqs = fasta_to_sequence( rfam_uri( rfam_id ) )\n",
      "from itertools import tee\n",
      "seqs,seqs_=tee(seqs)\n",
      "iterable_pos = seqs\n",
      "from eden.modifier.seq import seq_to_seq, shuffle_modifier\n",
      "iterable_neg = seq_to_seq( seqs_, modifier=shuffle_modifier, times=times, order=2 )\n",
      "\n",
      "#consier only first 'size' elements\n",
      "from itertools import islice\n",
      "iterable_pos = islice(iterable_pos,size)\n",
      "iterable_neg = islice(iterable_neg,size*times)\n",
      "\n",
      "#split train/test\n",
      "from eden.util import random_bipartition_iter\n",
      "iterable_pos_train, iterable_pos_test = random_bipartition_iter(iterable_pos, relative_size=train_test_split)\n",
      "iterable_neg_train, iterable_neg_test = random_bipartition_iter(iterable_neg, relative_size=train_test_split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#make predictive model\n",
      "from eden.model import ActiveLearningBinaryClassificationModel\n",
      "model = ActiveLearningBinaryClassificationModel( pre_processor, estimator=estimator, vectorizer=vectorizer )\n",
      "\n",
      "#optimize hyperparameters and fit model\n",
      "from numpy.random import randint\n",
      "from numpy.random import uniform\n",
      "pre_processor_parameters={'max_num':[3], \n",
      "                          'shape_type':[5], \n",
      "                          'energy_range':[30]}\n",
      "\n",
      "vectorizer_parameters={'complexity':[2]}\n",
      "\n",
      "estimator_parameters={'n_iter':randint(5, 100, size=n_iter),\n",
      "                      'penalty':['l1','l2','elasticnet'],\n",
      "                      'l1_ratio':uniform(0.1,0.9, size=n_iter), \n",
      "                      'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
      "                      'power_t':uniform(0.1, size=n_iter),\n",
      "                      'alpha': [10**x for x in range(-8,0)],\n",
      "                      'eta0': [10**x for x in range(-4,-1)],\n",
      "                      'learning_rate': [\"invscaling\", \"constant\", \"optimal\"]}\n",
      "active_set_size = size * 2\n",
      "model_fname='eden_model_active_%s'%rfam_id\n",
      "model.optimize(iterable_pos_train, iterable_neg_train, \n",
      "               model_name=model_fname,\n",
      "               score_func=lambda avg_score,std_score : avg_score - std_score * 2,\n",
      "               scoring='roc_auc',\n",
      "               n_active_learning_iterations=4,\n",
      "               max_total_time=60*30, n_iter=n_iter, \n",
      "               size_positive=-1,\n",
      "               size_negative=active_set_size,\n",
      "               cv=5, verbose=2,\n",
      "               pre_processor_parameters=pre_processor_parameters, \n",
      "               vectorizer_parameters=vectorizer_parameters, \n",
      "               estimator_parameters=estimator_parameters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--------------------------------------------------------------------------------\n",
        "Parameters range:\n",
        "Pre_processor:\n",
        "{'energy_range': [30], 'max_num': [3], 'shape_type': [5]}\n",
        "Vectorizer:\n",
        "{'complexity': [2]}\n",
        "Estimator:\n",
        "{'alpha': [1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1],\n",
        " 'eta0': [0.0001, 0.001, 0.01],\n",
        " 'l1_ratio': array([ 0.47446594,  0.48827839,  0.63909803,  0.77059824,  0.57662355,\n",
        "        0.82671952,  0.30389849,  0.73232795,  0.28938868,  0.67703373,\n",
        "        0.88015828,  0.84384397,  0.42796006,  0.34018379,  0.37188813,\n",
        "        0.88726631,  0.1481596 ,  0.15285235,  0.47606812,  0.87719968,\n",
        "        0.44693037,  0.2106537 ,  0.52407637,  0.45864127,  0.1737458 ,\n",
        "        0.66226666,  0.57864305,  0.73334573,  0.89769338,  0.32244311,\n",
        "        0.21577395,  0.12015794,  0.77649822,  0.35884168,  0.73651021,\n",
        "        0.16168334,  0.79349421,  0.69029553,  0.87438613,  0.87211217]),\n",
        " 'learning_rate': ['invscaling', 'constant', 'optimal'],\n",
        " 'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        " 'n_iter': array([91, 97, 88, 27, 53, 79, 64, 23, 67, 58, 25, 90, 63, 42, 76, 73, 18,\n",
        "       28,  5, 88, 67, 89, 97, 43,  8, 77, 76, 41, 22, 17,  7, 41,  7, 32,\n",
        "       43, 96, 60, 47, 24, 33]),\n",
        " 'penalty': ['l1', 'l2', 'elasticnet'],\n",
        " 'power_t': array([ 0.26348884,  0.53102081,  0.13211054,  0.90879203,  0.16473903,\n",
        "        0.59872638,  0.54854604,  0.17397677,  0.69279799,  0.22603448,\n",
        "        0.64365318,  0.20803705,  0.58687182,  0.8366009 ,  0.65343954,\n",
        "        0.41360131,  0.95578433,  0.84307701,  0.94124898,  0.38546332,\n",
        "        0.54975041,  0.70223921,  0.77266101,  0.97181414,  0.22416485,\n",
        "        0.77617166,  0.34447535,  0.99767344,  0.72862757,  0.77745232,\n",
        "        0.52166127,  0.69833092,  0.72579734,  0.10153224,  0.86933113,\n",
        "        0.53959579,  0.46281331,  0.40925114,  0.34788062,  0.8709658 ])}\n",
        "--------------------------------------------------------------------------------\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-13-6137c4d4756f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'#make predictive model\\nfrom eden.model import ActiveLearningBinaryClassificationModel\\nmodel = ActiveLearningBinaryClassificationModel( pre_processor, estimator=estimator, vectorizer=vectorizer )\\n\\n#optimize hyperparameters and fit model\\nfrom numpy.random import randint\\nfrom numpy.random import uniform\\npre_processor_parameters={\\'max_num\\':[3], \\n                          \\'shape_type\\':[5], \\n                          \\'energy_range\\':[30]}\\n\\nvectorizer_parameters={\\'complexity\\':[2]}\\n\\nestimator_parameters={\\'n_iter\\':randint(5, 100, size=n_iter),\\n                      \\'penalty\\':[\\'l1\\',\\'l2\\',\\'elasticnet\\'],\\n                      \\'l1_ratio\\':uniform(0.1,0.9, size=n_iter), \\n                      \\'loss\\':[\\'hinge\\', \\'log\\', \\'modified_huber\\', \\'squared_hinge\\', \\'perceptron\\'],\\n                      \\'power_t\\':uniform(0.1, size=n_iter),\\n                      \\'alpha\\': [10**x for x in range(-8,0)],\\n                      \\'eta0\\': [10**x for x in range(-4,-1)],\\n                      \\'learning_rate\\': [\"invscaling\", \"constant\", \"optimal\"]}\\nactive_set_size = size * 2\\nmodel_fname=\\'eden_model_active_%s\\'%rfam_id\\nmodel.optimize(iterable_pos_train, iterable_neg_train, \\n               model_name=model_fname,\\n               score_func=lambda avg_score,std_score : avg_score - std_score * 2,\\n               scoring=\\'roc_auc\\',\\n               n_active_learning_iterations=4,\\n               max_total_time=60*30, n_iter=n_iter, \\n               size_positive=-1,\\n               size_negative=active_set_size,\\n               cv=5, verbose=2,\\n               pre_processor_parameters=pre_processor_parameters, \\n               vectorizer_parameters=vectorizer_parameters, \\n               estimator_parameters=estimator_parameters)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2162\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2163\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/Users/costa/Desktop/BTSync/Projects/EDeN/EDeN/eden/model.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, iterable_pos, iterable_neg, model_name, n_active_learning_iterations, size_positive, size_negative, lower_bound_threshold_positive, upper_bound_threshold_positive, lower_bound_threshold_negative, upper_bound_threshold_negative, n_iter, max_total_time, fit_vectorizer, pre_processor_parameters, vectorizer_parameters, estimator_parameters, verbose, cv, scoring, score_func)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0mscore_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mscore_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                 \u001b[0;31m# update the best confirguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbest_score_\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    562\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         self._partial_fit(X, y, alpha, C, loss, learning_rate, self.n_iter,\n\u001b[0;32m--> 429\u001b[0;31m                           classes, sample_weight, coef_init, intercept_init)\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, n_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    389\u001b[0m             self._fit_binary(X, y, alpha=alpha, C=C,\n\u001b[1;32m    390\u001b[0m                              \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              sample_weight=sample_weight, n_iter=n_iter)\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             raise ValueError(\"The number of class labels must be \"\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(self, X, y, alpha, C, sample_weight, learning_rate, n_iter)\u001b[0m\n\u001b[1;32m    438\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expanded_class_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expanded_class_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                                      sample_weight)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36mfit_binary\u001b[0;34m(est, i, X, y, alpha, C, learning_rate, n_iter, pos_weight, neg_weight, sample_weight)\u001b[0m\n\u001b[1;32m    304\u001b[0m                                             \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                                             \u001b[0mintercept_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                                             est.average)\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#estimate predictive performance\n",
      "model.estimate( iterable_pos_test, iterable_neg_test )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from eden.model import ActiveLearningBinaryClassificationModel\n",
      "\n",
      "model2 = ActiveLearningBinaryClassificationModel()\n",
      "model2.load(model_fname)\n",
      "\n",
      "from eden.converter.fasta import fasta_to_sequence\n",
      "seqs = fasta_to_sequence( rfam_uri( rfam_id ) )\n",
      "from itertools import tee\n",
      "seqs,seqs_=tee(seqs)\n",
      "iterable_pos = seqs\n",
      "\n",
      "#consier only first 'size' elements\n",
      "from itertools import islice\n",
      "iterable_pos = islice(iterable_pos,size)\n",
      "\n",
      "predictions= model2.decision_function( iterable_pos )\n",
      "for n,i in enumerate(sorted(predictions)): print n,i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}